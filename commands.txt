data_cleaning
data
Data I/O
Read and write .csv
import delimited using "https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv", clear \nexport delimited using "flightdata.csv", replace
dat = fread('https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv')\nfwrite(dat, 'flightdata.csv')

data_cleaning
data
Data I/O
Read and write .parquet
* Stata currently has limited support for parquet files \n* (and Linux/Unix only). \n* See: https://github.com/mcaceresb/stata-parquet
# These commands require the `arrow` package \npfiles = dir(pattern = ".parquet") \nrbindlist(lapply(pfiles, arrow::read_parquet)) \nrbindlist(lapply(pfiles, arrow::read_parquet, col_select=1:10)

data_cleaning
data
Data I/O
Read and write .dta
* .dta is Stata's native (proprietary) filetype \nuse "filename.dta", clear \n\n\nsave "filename.dta", replace
# These commands require the `haven` package \ndat = haven::read_dta('filename.dta') \nsetDT(dat) # Or: dat = as.data.table(dat) \n \nhaven::write_dta(dat, 'filename.dta')

data_cleaning
order
Order
Sort rows
sort air_time \nsort air_time dest \ngsort -air_time\n\n
setorder(dat, air_time) \nsetorder(dat, air_time, dest) \nsetorder(dat, -air_time)

data_cleaning
order
Order
Sort columns
order month day
setcolorder(dat, c('month','day'))

data_cleaning
order
Order
Rename columns
* rename (old) (new) \n\nrename arr_delay arrival_delay \nrename (carrier origin) (carrier_code origin_code) \nrename arr_* arrival_*
# setnames(dat, old = ..., new = ...) \n\nsetnames(dat, 'arr_delay', 'arrival_delay') \nsetnames(dat, c('carrier','origin'), c('carrier_code','origin_code')) \nsetnames(dat, gsub('arr_', 'arrival_', names(dat)))

data_cleaning
subset
Subset
Subset rows
* Reminder: You'll need to use preserve/restore\n* if you want to retain the original dataset in \n* the examples that follow. \n\nkeep in 1/200 \nkeep if day > 5 & day < 10\nkeep if inrange(day,5,10)\nkeep if origin == "LGA"\nkeep if regex(origin,"LGA") \nkeep if inlist(month,3,4,11,12) \nkeep if inlist(origin,"JFK","LGA") \ndrop if month == 1
# Reminder: You'll need to (re)assign the \n# collapsed dataset if you want to use it later,\n# e.g. dat1 = dat[1:200] \n\ndat[1:200] \ndat[day > 5 & day < 10] \ndat[between(day,5,10)] # Or: dat[day %in% 5:10] \ndat[origin=='LGA']\ndat[origin %like% 'LGA'] \ndat[month %in% c(3,4,11,12)] \ndat[origin %chin% c("JFK","LGA")] # %chin% is a faster %in% for (ch)aracter strings \ndat[month!=1]

data_cleaning
subset
Subset
Subset columns
* Reminder: You'll need to use preserve/restore\n* if you want to retain the original dataset in \n* the examples that follow. \n\nkeep month day carrier \n\n\n\nkeep year-arr_delay\nkeep *_delay \n\ndrop origin dest \n\n\nds, has(type string) \ndrop `r(varlist)' \n\nds, has(type int) \nkeep `r(varlist)'
# Reminder: You'll need to (re)assign the \n# collapsed dataset if you want to use it later,\n# e.g. dat1 = dat[, .(month, day, carrier)] \n\ndat[, .(month, day, carrier)] \ndat[, list(month, day, carrier)] # same as above \ndat[, c('month', 'day', 'carrier')] # ditto \n\ndat[, year:arr_delay] \ndat[, .SD, .SDcols=patterns('*_delay')] \n\ndat[, -c('origin', 'dest')]\ndat[, c('origin', 'dest') := NULL] # same, but in-place \n\n# Matches the two lines on the left:\ndat[, .SD, .SDcols=!is.character] \n\n# Matches the two lines on the left: \ndat[, .SD, .SDcols=is.integer]

data_cleaning
subset
Subset
Subset rows and columns
* Reminder: You'll need to use preserve/restore\n* if you want to retain the original dataset in \n* the examples that follow. \n\nkeep if origin == "LGA"\nkeep month day carrier
# Reminder: You'll need to (re)assign the \n# collapsed dataset if you want to use it later,\n# e.g. dat1 = dat[origin=="LGA", .(month, day, carrier)] \n\n# Matches the two lines on the left:\ndat[origin=="LGA", .(month, day, carrier)]

data_cleaning
subset
Subset
Drop duplicates
* Reminder: You'll need to use preserve/restore\n* if you want to retain the original dataset in \n* the examples that follow. \n\nduplicates drop\nduplicates drop month day carrier, force\n
# Reminder: You'll need to (re)assign the \n# collapsed dataset if you want to use it later,\n# e.g. dat1 = unique(dat) \n\nunique(dat) \nunique(dat, by = c('month', 'day', 'carrier'))

data_cleaning
subset
Subset
Drop missing
* Reminder: You'll need to use preserve/restore\n* if you want to retain the original dataset in \n* the examples that follow. \n\nkeep if !missing(dest)\n* Requires: ssc inst missings\nmissings dropvars, force \nmissings air_time dest, force \n
# Reminder: You'll need to (re)assign the \n# collapsed dataset if you want to use it later,\n# e.g. dat = dat[!is.na(dest)] \n\ndat[!is.na(dest)]\n\nna.omit(dat) \nna.omit(dat, cols = c('air_time', 'dest')) \ndat[!is.na(air_time) & !is.na(dest)] # Same as above

data_cleaning
modify
Modify
Create new variables
gen dist_sq = distance^2 \ngen tot_delay = dep_delay + arr_delay \ngen first_letter = substr(origin, 1,1) \ngen flight_path = origin + '_' + dest \n\n* These next operations don't have a great Stata \n* equivalent, although you could implement a loop.
dat[, dist_sq := distance^2] \ndat[, tot_delay := dep_delay + arr_delay] \ndat[, first_letter := substr(origin,1,1)] \ndat[, flight_path := paste(origin, dest, sep='_')] \n\n# Multiple variables can be created at once.\n# These next few lines all do the same thing.\n# Just pick your favourite. \ndat[, c('dist_sq', 'dist_cu') := .(distance^2, distance^3)] \ndat[, ':=' (dist_sq=distance^2, dist_cu=distance^3)] # "functional" equivalent \ndat[, let(dist_sq=distance^2, dist_cu=distance^3)] # dev version only\n\n# We can also chain back-to-back dat[...][...] \n# (this holds for any data.table operation) \ndat[, dist_sq := distance^2][\n    , dist_cu := distance*dist_sq)]

data_cleaning
modify
Modify
Create new variables within groups
bysort origin: egen mean_dep_delay = mean(dep_delay) \nbysort origin dest: egen mean_dep_delay2 = mean(dep_delay) \n\n* Multiple grouped variables (manual demean example) \nforeach x of varlist dep_delay arr_delay air_time {\n    egen mean_`x'=mean(`x'), by(origin) \n    gen `x'_dm = `x` - mean_`x' \n    drop mean* \n}\n\n* Some short-cut symbols \nbysort carrier: g rows_per_carrier = _N \nbysort carrier: g index_within_carrier = _n \negen origin_index = group(origin)\n\n* Refer to other rows (uses generic data set)\nsort group time\nby group: gen growth = X/X[_n-1]\nby group: gen growth_since_first = X/X[1]
dat[, mean_dep_delay := mean(dep_delay), by=origin] \ndat[, mean_dep_delay2 := mean(dep_delay), by=.(origin, dest)] \n\n# Multiple grouped variables (manual demean example) \ndmcols = c('dep_delay', 'arr_delay', 'air_time') \ndat[,\n    paste0(dmcols,'_dm') := lapply(.SD, \(x) x-mean(x)),  # before R 4.1 you'll need function(x) instead of the \(x) shorthand\n    .SDcols = dmcols,\n    by = origin] \n\n# Some short-cut symbols \ndat[, rows_per_carrier := .N, by = carrier] \ndat[, index_within_carrier := .I, by = carrier] \ndat[, origin_index := .GRP, by = origin]\n\n# Refer to other rows (uses generic data set)\nsetorder(dat, group, time)\ndat[, growth := X/shift(X, 1), by = group]\ndat[, growth_since_first := X/first(X), by = group]

data_cleaning
modify
Modify
Work with dates
* Give ourselves a date variable to work with\ntostring year month day, replace\ngen day_string = year + "/" + month + "/" + day\ngen date = date(day_string, "YMD")\nformat date %td\n\n* Pull out year (quarter, month, etc. work too)\ngen the_year = year(date)\n\n* Shift forward 7 days\nreplace date = date + 7
# Make ourselves a date variable\ndat[, date := as.IDate(paste(year, month, day, sep='-'))] \n\n\n\n\n# Pull out year (quarter, month, etc. work too)\ndat[, the_year := year(date)]\n\n# Shift forward 7 days\ndat[, date := date + 7]


data_cleaning
modify
Modify
Modify existing variables
replace tot_delay = dep_delay + arr_delay \n\n* Conditional modification \nreplace distance = distance + 1 if month==9\nreplace distance = 0 in 1 \n\n* Modify multiple variables (same function) \nforeach x of varlist origin dest {\n    replace `x' = `x' + " Airport"\n}
dat[, tot_delay := dep_delay + arr_delay] \n\n# Conditional modification \ndat[month==9, distance := distance + 1]\ndat[1, distance := 0]\n\n# Modify multiple variables (same function) \ncols = c('origin','dest')\ndat[, (cols) := lapply(.SD, \(x) paste(x,'Airport')),  ## Note: before R 4.1 you need function(x) instead of the \(x) shorthand \n    .SDcols = cols] \n\n# Aside: We don't normally use a gen -> replace \n# workflow in R, the way we do in Stata. See the \n# 'Using Booleans & control-flow' section below.

data_cleaning
modify
Modify
Using Booleans & control-flow
gen long_flight = air_time>500 & !missing(air_time) \n\ngen flight_length = "Long" if air_time>500 & !missing(air_time)\nreplace flight_length = "Short" if missing(flight_length) & !missing(air_time) \n\n\ngen flight_length2 = "Long" if !missing(air_time) \nreplace flight_length2 = "Med" if air_time<=500  \nreplace flight_length2 = "Short" if air_time<=120
dat[, long_flight := air_time>500] \n\ndat[, flight_length := fifelse(air_time>500, 'Long', 'Short')] \n# fifelse is like base-R ifelse, but (f)aster! \n\n# for nested ifelse, easier to use fcase \ndat[, flight_length2 := fcase(air_time<=120, 'Short', \n                              air_time<=500, 'Med', \n                              default = 'Long')]

data_cleaning
modify
Modify
Row-wise calculations
* Pre-packaged row calculations: \negen tot_delay = rowtotal(*_delay)\negen any_delay = rowfirst(*_delay)\n\n* Custom row calculations:\n* ?
# Pre-packaged row calculations: \ndat[, tot_delay := rowSums(.SD), .SDcols = patterns('*_delay')]\ndat[, any_delay := fcoalesce(.SD), .SDcols = patterns('*_delay')] \n\n# Custom row calculations: \ndat[, new_var := mapply(custom_func, var1, var2)] \ndat[, new_var := custom_func(var1, var2)), by=.I] # dev version only\n\n

data_cleaning
modify
Modify
Fill in Time Series/Panel Data
* Carry forward the last-known observation\nsort id time\nby id: replace x = x[_n-1] if missing(x)\n* Carry back the next-known observation\ngsort id -time\nby id: replace x = x[_n-1] if missing(x)
# Carry forward the last-known observation\nsetorder(dat, id, time)\ndat[, x := nafill(x, type = 'locf'), by = id]\n# Carry back the next-known observation\ndat[, x := nafill(x, type = 'nocb'), by = id]

data_cleaning
collapse
Collapse
Collapse with no grouping
* Reminder: You'll need to use preserve/restore\n* if you want to retain the original dataset in \n* the examples that follow. \n\ncollapse (mean) dep_delay \ncollapse (mean) mean_ddel=dep_delay \n\ncollapse (mean) mean_ddel=dep_delay mean_adel=arr_delay \n\n\n\ncollapse (mean) *delay \n\nds, has(type long)\ncollapse (mean) `r(varlist)'
# Reminder: You'll need to (re)assign the \n# collapsed dataset if you want to use it later,\n# e.g. dat1 = dat[, mean(dep_delay)] \n\ndat[, mean(dep_delay)] # Just give me the number! As a scalar. \ndat[, .(mean_ddel=mean(dep_delay))] # Give me back a data.table (note the .() here, that's what does it) \n\ndat[, .(mean_ddel=mean(dep_delay), mean_adel=mean(arr_delay))]\ndat[, lapply(.SD, mean), .SDcols=c('arr_delay','dep_delay')] # same \ndat[, lapply(.SD, mean), .SDcols=arr_delay:dep_delay] # ditto \n\ndat[, lapply(.SD, mean), .SDcols=patterns('delay')] \n\n # Matches the two lines on the left\ndat[, lapply(.SD, mean), .SDcols=is.numeric]

data_cleaning
collapse
Collapse
Collapse by group
* Reminder: You'll need to use preserve/restore\n* if you want to retain the original dataset in \n* the examples that follow. \n\ncollapse (mean) arr_delay, by(carrier) \ncollapse (mean) mean_adel = arr_delay, by(carrier) \n\ncollapse (mean) arr_delay, by(carrier month) \n\ncollapse (min) min_d = distance (max) max_d = distance, by(origin) \n\ncollapse (mean) *_delay, by(origin) \ncollapse (mean) dep_delay arr_delay air_time distance, by(origin) \n\n\negen unique_dest = tag(dest origin) \ncollapse (sum) unique_dest, by(origin)
# Reminder: You'll need to (re)assign the \n# collapsed dataset if you want to use it later,\n# e.g. dat1 = dat[, mean(dep_delay), by=origin] \n\ndat[, .(arr_delay = mean(arr_delay)), by=carrier] \ndat[, .(mean_adel = mean(arr_delay)), by=carrier] \n\ndat[, .(arr_delay = mean(arr_delay)), by=.(carrier, month)] \n\ndat[, .(min_d = min(distance), max_d = max(distance)), by=origin] \n\ndat[, lapply(.SD, mean), .SDcols=patterns('_delay'), by=origin] \ndat[, lapply(.SD, mean), .SDcols=c('dep_delay','arr_delay','air_time','distance'), by=origin] \ndat[, lapply(.SD, mean), .SDcols = c(4,5,9,10), by=origin] # same as above \n\n# Matches the final two lines on the left: \ndat[, .(unique_dest = uniqueN(dest)), by = origin] \n\n# Bonus: You can also do complicated (grouped)\n# aggregations as part of a dcast (i.e. reshape \n# wide) call. E.g. Get the min, mean and max\n# departure and arrival delays, by origin airport.\ndcast(dat, origin~., fun=list(min,mean,max),\n      value.var=c('dep_delay','arr_delay'))

data_cleaning
collapse
Collapse
Count rows
count\ncount if month==10\n* Count rows by group:\ntabulate origin
dat[, .N] # Or: nrow(dat) \ndat[month==10, .N] # Or: nrow(dat[month==10]\n# Count rows by group:\ndat[, .N, by = origin])

data_cleaning
collapse
Collapse
Grouped calculations and complex objects inside a data.table
* ??
# data.tables support list columns, so you can have \n# complex objects like regression models inside a \n# data.table. Among many other things, this means you \n# can nest simulations inside a data.table as easily  \n# as you would perform any other (grouped) operation. \n\n# Example: Grouped regression \n\n# Let's run a separate regression of arrival delay on \n# departure delay for each month, inside our dataset \n\n# Just the coefficients\ndat[,\n    .(beta = coef(lm(arr_delay ~ dep_delay, .SD))[2]),\n    by = month]\n\n# Keep the whole model for each month\nmods = dat[,\n           .(mod = list(lm(arr_delay ~ dep_delay, .SD))),\n           by = month] \n# Now you can do things like put all 10 models in a \n# regression table or coefficient plot \nmodelsummary::msummary(mods$mod) \nmodelsummary::modelplot(mods$mod, coef_omit = 'Inter')

data_cleaning
reshape
Reshape
Reshape prep (this dataset only)
* We'll generate row IDs to avoid the (reshape) ambiguity \n* of repeated entries per date \ngen id = _n \n\n* For the Stata reshape, it's also going to prove \n* convenient to rename the delay vars. \nrename (dep_delay arr_delay) (delay_dep delay_arr)
# We'll generate row IDs to avoid the (reshape) ambiguity \n# of repeated entries per date \ndat[, id := .I] 

data_cleaning
reshape
Reshape
Reshape long
reshape long delay_, i(id) j(delay_type) s
ldat = melt(dat, measure=patterns('_delay'))\n\n# Aside: you can also choose different names for your\n# new reshaped columns if you'd like, e.g. \nmelt(dat, measure=patterns('_delay'), variable='d_type')

data_cleaning
reshape
Reshape
Reshape wide
* This starts with the reshaped-long data from above\nreshape wide delay_, i(id) j(delay_type) s
# This starts with the reshaped-long data from above\nwdat = dcast(ldat, ... ~ variable)\n\n# Aside 1: If you only want to keep the id & *_delay cols\ndcast(ldat, id ~ variable)\n\n# Aside 2: It's also possible to perform complex and \n# powerful data aggregating tasks as part of the dcast \n# (i.e. reshape wide) call.\ndcast(dat, origin~., fun=list(min,mean,max),\n      value.var=c('dep_delay','arr_delay'))

data_cleaning
merge
Merge
Import and prep secondary dataset on airport characterists
import delimited using "https://vincentarelbundock.github.io/Rdatasets/csv/nycflights13/airports.csv", clear\n* Stata requires that merge ID variables have the same \n* name across datasets. \nrename faa dest \nsave dat2.dta, replace\nimport delimited using "https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv", clear
dat2 = fread("https://vincentarelbundock.github.io/Rdatasets/csv/nycflights13/airports.csv") \n# R _doesn't_ require that merge ID variables share the \n# same name across datasets. But we'll add this anyway.\ndat2[, dest := faa]

data_cleaning
merge
Merge
Inner merge (i.e. keep row matches only)
merge m:1 dest using dat2.dta, keep(3) nogen
mdat = merge(dat, dat2, by='dest') 

data_cleaning
merge
Merge
Full merge (i.e. keep all rows)
merge m:1 dest using dat2.dta, nogen
mdat = merge(dat, dat2, by='dest', all=TRUE)

data_cleaning
merge
Merge
Left merge (i.e. keep all rows from "main" dataset)
merge m:1 dest using dat2.dta, keep(1 3) nogen
mdat = merge(dat, dat2, by='dest', all.x=TRUE)

data_cleaning
merge
Merge
Right merge (i.e. keep all rows from "secondary" dataset)
merge m:1 dest using dat2.dta, keep(2 3) nogen
mdat = merge(dat, dat2, by='dest', all.y=TRUE)

data_cleaning
merge
Merge
Anti merge (i.e. keep non-matched rows only)
merge m:1 dest using dat2.dta, keep(1 2) nogen
mdat = dat[!dat2, on='dest']

data_cleaning
merge
Merge
Advanced merges (tips and tricks)
* These next options don't really have good Stata \n* equivalents (that we're aware of)
# Merge on different ID names \nmdat = merge(dat, dat2, by.x='dest', by.y='faa') \n\n# Set keys for even faster merges and syntax shortcuts \nsetkey(dat, dest); setkey(dat2, dest) \nmdat = merge(dat, dat2) ## note: don't need 'by' \n\n# Non-equi joins \n# Non-equi joins are a bit hard to understand if you've \n# never seen them before. But they are incredibly \n# powerful and solve a suprisingly common problem: \n# Merging datasets over a range (e.g. start to end \n# dates), rather than exact matches. \n# Simple example where we want to subset the 1st qtr \n# flights for American Airlines and 2nd qtr flights \n# for United Airlines: \ndat3 = data.table(carrier     = c('AA', 'UA'),\n                  start_month = c(1, 4),\n                  end_month   = c(3, 6)) \ndat[dat3, on = .(carrier,\n                 month >= start_month,\n                 month <= end_month)] \n\n# Rolling joins are similar and allow you to match a set\n# of dates forwards or backwards. For example, our `dat` \n# datset ends in October. Let's say we want to carry the \n# last known entries for American and United Airlines \n# forward to (random) future dates. \ndat4 = data.table(carrier  = c('AA', 'UA'),\n                  new_date = as.IDate(c('2014-11-01',\n                                        '2014-11-15'))) \ndat[, date := as.IDate(paste(year, month, day, sep='-'))] \ndat[dat4, on = .(carrier, date=new_date), roll='nearest']

data_cleaning
merge
Merge
Appending data
* This just appends the flights data to itself\nsave data_to_append.dta, replace\nappend using data_to_append.dta
# This just appends the flights data to itself\nrbindlist(list(dat, dat)) # Or rbind(dat, dat)\n# The fill = TRUE option is handy if the one data set has columns the other doesn't

regression
models
Models
Simple model
reg wage educ \nreg wage educ age
feols(wage ~ educ, data = dat) \nfeols(wage ~ educ + age, data = dat)\n\n# Aside 1: `data = ...` is always the first argument \n# after the model formula. So many R users would just \n# write: \nfeols(wage ~ educ, dat) \n\n# Aside 2: You can also set your dataset globally so \n# that you don't have to reference it each time. \nsetFixest_estimation(data = dat) \nfeols(wage ~ educ) \nfeols(wage ~ educ + age) \n# etc.

regression
models
Models
Categorical variables
reg wage educ i.treat \n\n* Specifying a baseline:\nreg wage educ ib1.treat
feols(wage ~ educ + i(treat), dat) \n\n# Specifying a baseline:\nfeols(wage ~ educ + i(treat, ref = 1), dat)

regression
models
Models
Fixed effects
reghdfe wage educ, absorb(countyfips) cluster(countyfips) \n\n\n\n\n\nreghdfe wage educ, absorb(countyfips)  \n\n* Add more fixed effects... \nreghdfe wage educ, absorb(countyfips year) \\\ \n                   vce(cluster countyfips year) \nreghdfe wage educ, absorb(countyfips#year) \\\ \n                   vce(cluster countyfips#year)
feols(wage ~ educ | countyfips, dat) \n\n# Aside: fixest automatically clusters SEs by the first \n# fixed effect (if there are any). We'll get to SEs \n# later, but if you just want iid errors for a fixed \n# effect model: \nfeols(wage ~ educ | countyfips, dat, vcov = 'iid') \n\n# Add more fixed effects... \nfeols(wage ~ educ | countyfips + year, \n      dat, vcov = ~countyfips + year) \nfeols(wage ~ educ | countyfips^year, \n      dat) # defaults to vcov = ~countyfips^year

regression
models
Models
Instrumental variables
ivreg 2sls wage (educ = age) \nivreg 2sls wage mar (educ = age) \n\n* With fixed effects \nivreghdfe 2sls wage mar (educ = age), absorb(countyfips)
feols(wage ~ 1 | educ ~ age, dat)  \nfeols(wage ~ mar | educ ~ age, dat) \n\n# With fixed effects (IV 1st stage always comes last) \nfeols(wage ~ mar | countyfips | educ ~ age, dat)

regression
models
Models
Macros, wildcards and shortcuts
local ctrls age black hisp marr \nreg wage educ `ctrls' \n\nreg wage educ x* \nreg wage educ *sp  \nreg wage educ *ac*
ctrls = c("age", "black", "hisp", "marr") \nfeols(wage ~ educ + .[ctrls], dat) \n\nfeols(wage ~ educ + ..('^x'), dat) # ^ = starts with \nfeols(wage ~ educ + ..('sp$'), dat) # $ = ends with \nfeols(wage ~ educ + ..('ac'), dat) \n\n# Many more macro options. See `?setFixest_fml` and\n# `?setFixest_estimation`. Example (reminder) where \n# you set your dataset globally, so you don't have to \n# retype `data = ...` anymore. \nsetFixed_estimation(data = dat) \nfeols(wage ~ educ) \nfeols(wage ~ educ + .[ctrls] | statefips) \n# Etc.

regression
interactions
Interactions
Interact continuous variables
reg wage c.educ#c.age \nreg wage c.educ##c.age \n\n* Polynomials \nreg wage c.age#c.age \nreg wage c.age##c.age 
feols(wage ~ educ:age, dat) \nfeols(wage ~ educ*age, dat) \n\n# Polynomials \nfeols(wage ~ I(age^2), dat) \nfeols(wage ~ poly(age, 2, raw = TRUE))

regression
interactions
Interactions
Interact categorical variables
reg wage i.treat#i.hisp \n\n\n\n\nreg wage i.treat i.treat#i.hisp\nreg wage i.treat##i.hisp
feols(wage ~ i(treat, i.hisp), dat) \n\n# Aside: i() is a fixest-specific shortcut that also \n# has synergies with some other fixest functions. But \n# base R interaction operators all still work, e.g. \nfeols(wage ~ factor(treat)/factor(hisp), dat) \nfeols(wage ~ factor(treat)*factor(hisp), dat)

regression
interactions
Interactions
Interact categorical with continuous variables
reg wage i.treat#c.age \n\n\n\n\nreg wage i.treat#c.age \nreg wage i.treat i.treat#c.age \nreg wage i.treat##c.age
feols(wage ~ i(treat, age), dat) \n\n# Aside: i() is a fixest-specific shortcut that also \n# has synergies with some other fixest functions. But \n# base R interaction operators all still work, e.g. \nfeols(wage ~ factor(treat):age, dat) \nfeols(wage ~ factor(treat)/age, dat) \nfeols(wage ~ factor(treat)*age, dat)

regression
interactions
Interactions
Interact fixed effects
* Combine fixed effects \nreghdfe wage educ, absorb(statefips#year) \n\n* Varying slopes (e.g. time trend for each state) \n? 
# Combine fixed effects \nfeols(wage ~ educ | statefips^year, dat) \n\n# Varying slopes (e.g. time trend for each state) \nfeols(wage ~ educ | statefips[year], dat)

regression
std_errors
Standard errors
HC
reg wage educ, vce(robust) \nreg wage educ, vce(hc3)
feols(wage ~ educ, dat, vcov = 'hc1') \nfeols(wage ~ educ, dat, vcov = sandwich:vcovHC) \n\n# Note: You can also adjust the SEs of an existing model \nm = feols(wage ~ educ, dat) \nsummary(m, vcov = 'hc1')

regression
std_errors
Standard errors
HAC
xtset id year\nivreghdfe wage educ, bw(auto) vce(robust)
feols(y ~ x, dat, vcov = 'NW', panel.id = ~unit + time)\nfeols(wage ~ educ, dat, vcov = 'NW') # if panel id is already set (see below)

regression
std_errors
Standard errors
Clustered
reghdfe wage educ, absorb(countyfips) \\\ \n                   vce(cluster countyfips) \n\n* Twoway clustering etc. \nreghdfe wage educ, absorb(countyfips year) \\\ \n                   vce(cluster countyfips year) \n\n\nreghdfe wage educ, absorb(countyfips#year) \\\ \n                   vce(cluster countyfips#year)
feols(wage ~ educ | countyfips, dat) # Auto clusters by FE \n# feols(wage ~ educ | countyfips, dat, vcov = ~countyfips) # ofc can be explicit too \n\n# Twoway clustering etc. \nfeols(wage ~ educ | countyfips + year, \n      dat, vcov = ~countyfips + year) \n# feols(wage ~ educ | countyfips + year, \n#      dat, vcov = 'twoway') ## same as above \nfeols(wage ~ educ | countyfips^year, \n      dat, vcov = ~countyfips^year) \n\n# Reminder that you can adjust the SEs of existing \n# fixest models on-the-fly. \nm = feols(wage ~ educ | countyfips + year, dat) \nm # Clustered by countyfips (default) \nsummary(m, vcov = 'twoway') \nsummmary(m, vcov = ~countyfips^year) \n# etc.

regression
std_errors
Standard errors
Conley standard errors
* See: http://www.trfetzer.com/conley-spatial-hac-errors-with-fixed-effects/
feols(wage ~ educ, dat, vcov = conley("25 mi"))

regression
presentation
Presentation
Regression table
reg wage educ age \neststore est1 \nesttab est1\n\nreg wage educ age black hisp\neststore est2\nesttab est1 est2
est1 = feols(wage ~ educ + age, dat) \netable(est1)\n\n\nest2 = feols(wage ~ educ + age + black + hisp, dat) \netable(est1, est2)

regression
presentation
Presentation
Coefficient plot
reg wage educ age black hisp  \n eststore est1 \n coefplot ...
est1 = feols(wage ~ educ + age + black + hisp, dat) \n coefplot(est1)

regression
panel
Panel
Lag variables
xtset id year \n reg wage educ l1.wage
feols(wage ~ educ + l(wage, 1), dat, panel.id = ~id+year)

regression
panel
Panel
Lead variables
xtset id year \n reg wage educ f1.wage
feols(wage ~ educ + l(wage, -1), dat, panel.id = ~id+year)

regression
panel
Panel
First difference
xtset id year \n reg wage educ D.x
feols(wage ~ educ + d(wage), dat, panel.id = ~id+year)

misc
ggplot
ggplot2: Beautiful and customizable plots
Basic scatterplot
twoway scatter yvar xvar\n* "Graphing the same thing separately across groups" is absurdly easier in ggplot2 than in Stata\n twoway (scatter yvar xvar if group == 1, mc(blue)) (scatter yvar xvar if group == 2, mc(red))
ggplot(dat, aes(x = xvar, y = yvar)) + geom_point()\n# "Graphing the same thing separately across groups" is absurdly easier in ggplot2 than in Stata\nggplot(dat, aes(x = xvar, y = yvar, color = group)) + geom_point()

misc
tidyverse
tidyverse
Data wrangling with dplyr
* Subset by rows and then columns \nkeep if var1=="value" \nkeep var1 var2 var3 \n\n\n* Create a new variable by group \nbysort group1: egen mean_var1 = mean(var1) \n\n\n\n* Collapse by group \ncollapse (mean) arr_delay, by(carrier)
# Subset by rows and then columns \n dat %>%   # `%>%` is the tidyverse "pipe" operator\n   filter(var1=="value") %>%\n   select(var1, var2, var3) \n\n# Create a new variable by group \ndat %>% \n  group_by(group1) %>% \n  mutate(mean_var1 = mean(var1)) \n\n# Collapse by group \ndat %>% \n  group_by(group1) %>% \n  summarise(mean_var1 = mean(var1)) \n\n# PS. dplyr doesn't modify data in place. So you'll need\n# to (re)assign if you want to keep the above changes. \n# E.g. dat = dat %>% group_by...

misc
tidyverse
tidyverse
Manipulating dates with lubridate
* Shift a date forward one month (not 30 days, one month)\n* ??? 
# Shift a date forward one month (not 30 days, one month)\nshifted_date = date + months(1)

misc
tidyverse
tidyverse
Iterating with purrr
* Read in many files and append them together\nlocal filelist: dir "Data/" files "*.dta"\nlocal firsttime = 1\nforeach f in filelist {\n    use `f', clear\n    if `firsttime' == 0 {\n        append using compiled_data.dta\n    }\n    save compiled_data.dta, replace\n}
# Read in many files and append them together\n# (this combines purrr with the data.table function fread)\nfilelist = list.files('Data/', pattern = '.csv')\ndat = filelist %>%\n    map_df(fread)

misc
tidyverse
tidyverse
String operations with stringr
subinstr(string, "remove", "replace", .)\nsubstr(string, start, length)\nregex(string, "regex")
str_replace_all(string, "remove", "replace")\nstr_sub(string, start, end)\nstr_detect(string, "regex")\n# Note all the stringr functions accept regex input

misc
collapse 
collapse: Extra convenience functions and super fast aggregations
Quick summaries
summarize\ndescribe
qsu(dat)\ndescr(dat)

misc
collapse 
collapse: Extra convenience functions and super fast aggregations
Multiple grouped aggregations
collapse (mean) var1, by(group1)\ncollapse (min) min_var1=var1 min_var2=var2 (max) max_var1=var1 max_var2=var2, by(group1 group2)
collap(dat, var1 ~ group1, fmean) # 'fmean' => fast mean\ncollap(dat, var1 + var2 ~ group1 + group2, FUN = list(fmin, fmax))

misc
sandwich
sandwich: More standard error adjustments
Linear model adjustments
* ", robust" uses hc1 which isn't great for small samples\nregress Y X Z, vce(hc3)
# sandwich's vcovHC uses HC3 by default\nfeols(Y ~ X + Z, dat,vcov = sandwich::vcovHC) \n\n# Aside: Remember that you can also adjust the SEs \n# for existing models on the fly \nm = feols(Y ~ X + Z, dat) \nsummary(m, vcov = sandwich::vcovHC)

misc
modelsummary
modelsummary: Summary tables, regression tables, and more
Summary tables
* Summary stats table \nestpost summarize \nesttab, cells("count mean sd min max") nomtitle nonumber \n\n* Balance table \nby treat_var: eststo: estpost summarize \nesttab, cells("mean sd") label nodepvar
# Summary stats table \ndatasummary_skim(dat) \n\n\n# Balance table \ndatasummary_balance(~treat_var, dat)

misc
modelsummary
modelsummary: Summary tables, regression tables, and more
Regression tables
reg Y X Z \neststore est1 \nesttab est1b\n\nreg Y X Z, vce(hc3) \neststore est1b \nesttab est1b \n\nesttab est1 est1b\n\nreg Y X Z A, vce(hc3)\neststore est2\nesttab est1 est1b est2
est1 = lm(Y ~ X + Z, dat) \nmsummary(est1) # msummary() = alias for modelsummary()\n\n# Like fixest::etable(), SEs for existing models can\n# be adjusted on-the-fly \nmsummary(est1, vcov='HC3')\n\n# Multiple SEs for the same model\nmsummary(est1, vcov=list('iid', 'HC3')) \n\nest3 = lm(Y ~ X + Z + A, dat) \nmsummary(list(est1, est1, est3),\n         vcov = list('iid', 'HC3', 'HC3'))

misc
lme4
lme4: Random effects and mixed models
Random effects and mixed models
xtset group time\nxtreg Y X, re\nmixed lifeexp || countryn: gdppercap
# No need for an xtset equivalent\nm = lmer(Y~(1|group) + X, dat)\nnm = lmer(Y~(1+x|group) + X, dat)

misc
marginaleffects
marginaleffects: Marginal effects, constrasts, etc.
Basic logit marginal effects
* A logit:\nlogit Y X Z\nmargins, dydx(*)
# This example incorporates the fixest function feglm()\nm = feglm(Y ~ X + Z, family = binomial, data = mtcars)\nsummary(marginaleffects(m))

misc
multcomp
multcomp and nlWaldTest: Joint coefficient tests
Test other null hypotheses and coefficient combinations
\nregress y x z \n\n\n\n\n* One-sided test \ntest _b[x]=0 \nlocal sign_wgt = sign(_b[x]) \ndisplay "H0: coef <= 0  p-value = " ttail(r(df_r),`sign_wgt'*sqrt(r(F))) \n\n* Test linear combination of coefficients \nlincom x + z \n\n\n* Test nonlinear combination of coefficients \nnlcom _b[x]/_b[z]
m = feols(y ~ x + z, dat)\n\n# Note: we recommend the dev version of multcomp \n# install.packages("multcomp", repos="http://R-Forge.R-project.org") \n\n# One-sided test \nm2 = multcomp::ghlt(m, 'x<=0')\nsummary(m2) \n\n\n# Test linear combination of coefficients \nm3 = multcomp::glht(m, 'x + z = 0') \nsummary(m3) # or confint(m3) \n\n# Test nonlinear combination of coefficients \nnlWaldtest::nlWaldtest(m, 'b[2]/b[3]') # or nlWaldtest::nlConfint()

misc
sf
sf: Geospatial operations
Simple map
* Mapping in Stata requires the spmap and shp2dta \n* commands, and also that you convert your (say) \n* shapefile to .dta format first. We won't go through \n* all that here, but see: \n* https://www.stata.com/support/faqs/graphics/spmap-and-maps/
# This example uses the North Carolina shapefile that is\n# bundled with the sf package. \nnc = st_read(system.file("shape/nc.shp", package = "sf")) \nplot(nc[, 'BIR74'])\n# Or, if you have ggplot2 loaded: \nggplot(nc, aes(fill=BIR74)) + geom_sf()
